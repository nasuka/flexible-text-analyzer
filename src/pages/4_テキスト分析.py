import json
import os

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

from schema.llm_providers import LLMModel, LLMProvider
from services.text_analyzer import (
    AnalysisSettings,
    TextAnalyzer,
    create_markdown_report,
)
from services.text_column_estimator import (
    estimate_text_column,
    get_text_column_recommendations,
)


def filter_data_by_query(
    df: pd.DataFrame, text_column: str, metadata_columns: list[str], query: str
) -> pd.DataFrame:
    """è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    # ã‚ˆã‚Šé«˜åº¦ãªå®Ÿè£…ã§ã¯ã€LLMã‚’ä½¿ç”¨ã—ã¦æ¡ä»¶ã‚’è§£é‡ˆã™ã‚‹ã“ã¨ã‚‚å¯èƒ½

    if not query.strip():
        return df

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    keywords = [word.strip().lower() for word in query.split() if len(word.strip()) > 1]

    if not keywords:
        return df

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’æ§‹ç¯‰
    mask = pd.Series([False] * len(df))

    for keyword in keywords:
        # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã§ã®æ¤œç´¢
        text_mask = df[text_column].str.contains(keyword, case=False, na=False)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ã§ã®æ¤œç´¢
        meta_mask = pd.Series([False] * len(df))
        for col in metadata_columns:
            if col in df.columns:
                meta_mask |= (
                    df[col].astype(str).str.contains(keyword, case=False, na=False)
                )

        mask |= text_mask | meta_mask

    return df[mask]


def create_summary_visualization(summary_data: dict) -> tuple[go.Figure, go.Figure]:
    """è¦ç´„çµæœã®å¯è¦–åŒ–"""

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å¸ƒ
    if "keywords" in summary_data:
        keywords = summary_data["keywords"][:10]  # ä¸Šä½10å€‹
        fig_keywords = px.bar(
            x=[kw["count"] for kw in keywords],
            y=[kw["word"] for kw in keywords],
            orientation="h",
            title="ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸Šä½10ï¼‰",
            labels={"x": "å‡ºç¾å›æ•°", "y": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"},
        )
    else:
        fig_keywords = go.Figure()
        fig_keywords.add_annotation(
            text="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“",
            x=0.5,
            y=0.5,
            xref="paper",
            yref="paper",
            showarrow=False,
        )

    # æ„Ÿæƒ…åˆ†å¸ƒ
    if "sentiment_distribution" in summary_data:
        sentiment_dist = summary_data["sentiment_distribution"]
        fig_sentiment = px.pie(
            values=list(sentiment_dist.values()),
            names=list(sentiment_dist.keys()),
            title="æ„Ÿæƒ…åˆ†å¸ƒ",
        )
    else:
        fig_sentiment = go.Figure()
        fig_sentiment.add_annotation(
            text="æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“",
            x=0.5,
            y=0.5,
            xref="paper",
            yref="paper",
            showarrow=False,
        )

    return fig_keywords, fig_sentiment


def main():
    st.title("ğŸ” ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ")
    st.markdown("---")

    # LLM APIè¨­å®š
    st.header("ğŸ”‘ LLMè¨­å®š")

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
    provider_display_names = [provider.get_display_name() for provider in LLMProvider]
    selected_provider_display = st.selectbox(
        "LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼",
        provider_display_names,
        help="ä½¿ç”¨ã™ã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„",
    )

    # é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
    selected_provider = None
    for provider in LLMProvider:
        if provider.get_display_name() == selected_provider_display:
            selected_provider = provider
            break

    # APIã‚­ãƒ¼å…¥åŠ›
    api_key_label = f"{selected_provider.get_display_name()} API Key"
    api_key_help = f"{selected_provider.get_display_name()} APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"

    if selected_provider == LLMProvider.OPENAI:
        api_key = st.text_input(
            api_key_label,
            value=os.getenv("OPENAI_API_KEY", ""),
            type="password",
            help=api_key_help,
        )
    elif selected_provider == LLMProvider.OPENROUTER:
        api_key = st.text_input(
            api_key_label,
            value=os.getenv("OPENROUTER_API_KEY", ""),
            type="password",
            help=api_key_help,
        )
    else:
        api_key = st.text_input(
            api_key_label,
            type="password",
            help=api_key_help,
        )

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    available_models = LLMModel.get_models_by_provider(selected_provider)
    model_display_names = [model.get_display_name() for model in available_models]

    selected_model_display = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«é¸æŠ",
        model_display_names,
        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
    )

    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    selected_model = None
    for model in available_models:
        if model.get_display_name() == selected_model_display:
            selected_model = model
            break

    if not api_key:
        st.warning(
            f"âš ï¸ {selected_provider.get_display_name()} API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        return

    # ãƒ‡ãƒ¼ã‚¿å…¥åŠ›
    st.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        type=["csv"],
        key="analysis_csv",
    )

    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

        # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
        recommended_column, analysis = estimate_text_column(df)

        # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
        if recommended_column:
            st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

            with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                recommendations = get_text_column_recommendations(df, top_n=3)
                for i, rec in enumerate(recommendations):
                    col_name = rec["column"]
                    details = rec["details"]
                    st.write(f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})")
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}")
                    with col2:
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}")
                    with col3:
                        st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                    st.divider()

        # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
        default_index = 0
        if recommended_column and recommended_column in df.columns:
            default_index = df.columns.tolist().index(recommended_column)

        text_column = st.selectbox(
            "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
            options=df.columns.tolist(),
            index=default_index,
        )

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ã®é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯èƒ½ï¼‰
        other_columns = [col for col in df.columns if col != text_column]
        metadata_columns = st.multiselect(
            "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆåˆ†æã«å«ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ï¼‰",
            options=other_columns,
            default=[],
            help="ãƒˆãƒ”ãƒƒã‚¯ã€ã‚«ãƒ†ã‚´ãƒªã€æ„Ÿæƒ…ãªã©ã€åˆ†æã«å«ã‚ãŸã„è¿½åŠ æƒ…å ±ã‚’é¸æŠã—ã¦ãã ã•ã„",
        )

        # åˆ†æã‚¯ã‚¨ãƒªå…¥åŠ›
        st.header("ğŸ” åˆ†ææŒ‡ç¤º")

        analysis_query = st.text_area(
            "åˆ†æã—ãŸã„å†…å®¹ã‚’è‡ªç„¶è¨€èªã§å…¥åŠ›ã—ã¦ãã ã•ã„",
            value="",
            placeholder="""ä¾‹:
- ãƒã‚¸ãƒ†ã‚£ãƒ–ãªã‚³ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„ã—ã¦åˆ†æã—ã¦
- å•†å“ã®å“è³ªã«é–¢ã™ã‚‹æ„è¦‹ã‚’ã¾ã¨ã‚ã¦
- å¦å®šçš„ãªæ„Ÿæƒ…ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦è¦ç´„
- ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã®æ„è¦‹ã‚’ã¾ã¨ã‚ã¦""",
            height=120,
            help="å…·ä½“çš„ãªåˆ†ææŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€æ„Ÿæƒ…ã€ãƒˆãƒ”ãƒƒã‚¯ãªã©ã‚’æŒ‡å®šã§ãã¾ã™ã€‚",
        )

        if analysis_query.strip():
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
            st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿è¨­å®š")

            st.info(f"ğŸ“ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æå¯¾è±¡ã¨ã—ã¾ã™: {len(df)}ä»¶")

            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            with st.expander("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                display_columns = [text_column] + metadata_columns
                st.dataframe(
                    df[display_columns].head(10),
                    use_container_width=True,
                )

            # ãƒ‡ãƒ¼ã‚¿åˆ¶é™
            data_limit = st.slider(
                "åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ä»¶æ•°",
                min_value=1,
                max_value=len(df),
                value=min(100, len(df)),
                help="LLMã§åˆ†æã™ã‚‹ä»¶æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼ˆå¤šã™ãã‚‹ã¨APIåˆ¶é™ã«æ³¨æ„ï¼‰",
            )

            # åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            analysis_df = df.head(data_limit)

            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°äºˆæ¸¬ï¼ˆä»®ã®åˆ†æã‚µãƒ¼ãƒ“ã‚¹ã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®šï¼‰
            temp_settings = AnalysisSettings(
                provider=selected_provider,
                model=selected_model,
                api_key="dummy",  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®šã®ãŸã‚ã®ãƒ€ãƒŸãƒ¼
            )
            temp_analyzer = TextAnalyzer(temp_settings)
            estimated_tokens = temp_analyzer.estimate_tokens(
                analysis_df, text_column, metadata_columns
            )
            st.warning(
                f"ğŸ“Š äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {estimated_tokens:,} tokensï¼ˆAPIã®åˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼‰"
            )

            # åˆ†æå®Ÿè¡Œ
            if st.button("ğŸš€ åˆ†æå®Ÿè¡Œ", type="primary"):
                with st.spinner("ğŸ¤– LLMã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆåˆ†æä¸­..."):
                    # åˆ†æè¨­å®šã®ä½œæˆ
                    settings = AnalysisSettings(
                        provider=selected_provider,
                        model=selected_model,
                        api_key=api_key,
                        temperature=0.3,
                        max_tokens=3000,
                    )

                    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–
                    analyzer = TextAnalyzer(settings)

                    # åˆ†æå®Ÿè¡Œ
                    result = analyzer.analyze_text(
                        data=df,
                        text_column=text_column,
                        metadata_columns=metadata_columns,
                        query=analysis_query,
                        data_limit=data_limit,
                    )

                    if result:
                        st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")

                        # åˆ†æçµæœã®è¡¨ç¤º
                        st.header("ğŸ“Š åˆ†æçµæœ")

                        # åŸºæœ¬çµ±è¨ˆ
                        st.subheader("ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ")
                        col1, col2, col3, col4 = st.columns(4)

                        with col1:
                            st.metric("åˆ†æå¯¾è±¡ä»¶æ•°", result.stats["total_count"])
                        with col2:
                            st.metric("å¹³å‡æ–‡å­—æ•°", f"{result.stats['avg_length']:.0f}")
                        with col3:
                            st.metric(
                                "ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ†ã‚­ã‚¹ãƒˆæ•°", result.stats["unique_count"]
                            )
                        with col4:
                            st.metric("æœ€å¤§æ–‡å­—æ•°", result.stats["max_length"])

                        # LLMåˆ†æçµæœ
                        st.subheader("ğŸ¤– LLMåˆ†æçµæœ")
                        st.markdown(result.result)

                        # ãƒ‡ãƒ¼ã‚¿è©³ç´°
                        st.subheader("ğŸ“‹ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿è©³ç´°")
                        display_columns = [result.text_column] + result.metadata_columns
                        st.dataframe(
                            result.data[display_columns],
                            use_container_width=True,
                        )

                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                        st.session_state["analysis_result"] = {
                            "query": result.query,
                            "result": result.result,
                            "data": result.data,
                            "text_column": result.text_column,
                            "metadata_columns": result.metadata_columns,
                            "stats": result.stats,
                            "settings": {
                                "provider": result.settings.provider.get_display_name(),
                                "model": result.settings.model.get_display_name(),
                                "data_count": result.stats["total_count"],
                            },
                        }

                        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                        st.header("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

                        col1, col2, col3 = st.columns(3)

                        with col1:
                            # åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿CSV
                            csv_data = result.data.to_csv(
                                index=False, encoding="utf-8-sig"
                            )
                            st.download_button(
                                label="ğŸ“¥ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ (CSV)",
                                data=csv_data,
                                file_name="analysis_data.csv",
                                mime="text/csv",
                                key="download_analysis_data",
                            )

                        with col2:
                            # åˆ†æçµæœJSON
                            result_json = {
                                "query": result.query,
                                "analysis_result": result.result,
                                "stats": result.stats,
                                "settings": {
                                    "provider": result.settings.provider.get_display_name(),
                                    "model": result.settings.model.get_display_name(),
                                    "data_count": result.stats["total_count"],
                                    "text_column": result.text_column,
                                    "metadata_columns": result.metadata_columns,
                                },
                                "data": result.data.to_dict("records"),
                            }

                            json_str = json.dumps(
                                result_json, ensure_ascii=False, indent=2
                            )
                            st.download_button(
                                label="ğŸ“¥ åˆ†æçµæœ (JSON)",
                                data=json_str,
                                file_name="analysis_result.json",
                                mime="application/json",
                                key="download_analysis_result",
                            )

                        with col3:
                            # åˆ†æçµæœMarkdown
                            markdown_content = create_markdown_report(result)
                            st.download_button(
                                label="ğŸ“¥ åˆ†æçµæœ (Markdown)",
                                data=markdown_content,
                                file_name="analysis_result.md",
                                mime="text/markdown",
                                key="download_analysis_markdown",
                            )

                    else:
                        st.error("âš ï¸ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åˆ†æçµæœãŒã‚ã‚‹å ´åˆã®å†è¡¨ç¤º
    if "analysis_result" in st.session_state:
        saved_result = st.session_state["analysis_result"]

        st.header("ğŸ“Š å‰å›ã®åˆ†æçµæœ")
        st.info(f"åˆ†æã‚¯ã‚¨ãƒª: {saved_result['query']}")

        with st.expander("ğŸ¤– åˆ†æçµæœã‚’å†è¡¨ç¤º"):
            st.markdown(saved_result["result"])

    # ä½¿ç”¨æ–¹æ³•
    with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
        st.markdown("""
        ### ğŸ“– ä½¿ç”¨æ–¹æ³•
        1. **LLM API Key**ã®è¨­å®š
        2. **CSVãƒ•ã‚¡ã‚¤ãƒ«**ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        3. **ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ **ã®é¸æŠ
        4. **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ **ã®é¸æŠï¼ˆä»»æ„ï¼‰
        5. **åˆ†ææŒ‡ç¤º**ã‚’è‡ªç„¶è¨€èªã§å…¥åŠ›
        6. **åˆ†æå®Ÿè¡Œ**ã§çµæœã‚’ç¢ºèª

        ### ğŸ” åˆ†ææŒ‡ç¤ºã®ä¾‹
        - "ãƒã‚¸ãƒ†ã‚£ãƒ–ãªã‚³ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„ã—ã¦åˆ†æã—ã¦"
        - "å•†å“ã®å“è³ªã«é–¢ã™ã‚‹æ„è¦‹ã‚’ã¾ã¨ã‚ã¦"
        - "å¦å®šçš„ãªæ„Ÿæƒ…ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦è¦ç´„"
        - "ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æ"

        ### ğŸ“Š å‡ºåŠ›å†…å®¹
        - **è¦ç´„**: ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®æ¦‚è¦
        - **ä¸»è¦ãªç™ºè¦‹**: æ³¨ç›®ã™ã¹ãç‚¹
        - **è©³ç´°åˆ†æ**: å…·ä½“çš„ãªåˆ†æçµæœ
        - **æ¨å¥¨äº‹é …**: æ”¹å–„ææ¡ˆ

        ### ğŸ’¡ æ´»ç”¨ã‚·ãƒ¼ãƒ³
        - **é¡§å®¢ãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æ**: å•†å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®æ„è¦‹æŠ½å‡º
        - **ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢åˆ†æ**: æŠ•ç¨¿å†…å®¹ã®å‚¾å‘æŠŠæ¡
        - **ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆåˆ†æ**: è‡ªç”±è¨˜è¿°å›ç­”ã®è¦ç´„
        - **ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å£°ã®æ•´ç†

        ### âš™ï¸ æŠ€è¡“ä»•æ§˜
        - **LLMçµ±åˆ**: ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ
        - **è‡ªå‹•ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æŠ½å‡º
        - **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ´»ç”¨**: æ§‹é€ åŒ–æƒ…å ±ã®çµ„ã¿è¾¼ã¿
        - **çµæœå‡ºåŠ›**: CSV/JSONå½¢å¼å¯¾å¿œ
        """)


if __name__ == "__main__":
    main()
