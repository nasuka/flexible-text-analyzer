import json
import os

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

from schema.llm_providers import LLMModel, LLMProvider
from services.text_column_estimator import (
    estimate_text_column,
    get_text_column_recommendations,
)
from services.topic_classifier import LLMTopicClassifier


def create_classification_charts(df_classified: pd.DataFrame) -> tuple:
    """åˆ†é¡çµæœã®å¯è¦–åŒ–"""

    # ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ
    topic_counts = df_classified["main_topic_name"].value_counts()
    fig_topic_dist = px.pie(
        values=topic_counts.values, names=topic_counts.index, title="ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ"
    )

    # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆä¸Šä½10ï¼‰
    subtopic_counts = df_classified["subtopic_name"].value_counts().head(10)
    fig_subtopic_dist = px.bar(
        x=subtopic_counts.values,
        y=subtopic_counts.index,
        orientation="h",
        title="ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆä¸Šä½10ï¼‰",
        labels={"x": "ä»¶æ•°", "y": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"},
    )

    # ä¿¡é ¼åº¦åˆ†å¸ƒ
    fig_confidence = px.histogram(
        df_classified,
        x="confidence",
        nbins=20,
        title="åˆ†é¡ä¿¡é ¼åº¦åˆ†å¸ƒ",
        labels={"x": "ä¿¡é ¼åº¦", "y": "ä»¶æ•°"},
    )

    return fig_topic_dist, fig_subtopic_dist, fig_confidence


def create_topic_subtopic_matrix(df_classified: pd.DataFrame) -> go.Figure:
    """ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–"""

    crosstab = pd.crosstab(
        df_classified["main_topic_name"], df_classified["subtopic_name"]
    )

    fig = px.imshow(
        crosstab.values,
        labels={"x": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯", "y": "ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯", "color": "ä»¶æ•°"},
        x=crosstab.columns,
        y=crosstab.index,
        title="ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚",
    )

    return fig


def main():
    st.title("ğŸ“Š LLMãƒˆãƒ”ãƒƒã‚¯åˆ†é¡")
    st.markdown("---")

    # LLM APIè¨­å®š
    st.header("ğŸ”‘ LLMè¨­å®š")

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
    provider_display_names = [provider.get_display_name() for provider in LLMProvider]
    selected_provider_display = st.selectbox(
        "LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼",
        provider_display_names,
        help="ä½¿ç”¨ã™ã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„",
    )

    # é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
    selected_provider = None
    for provider in LLMProvider:
        if provider.get_display_name() == selected_provider_display:
            selected_provider = provider
            break

    # APIã‚­ãƒ¼å…¥åŠ›
    api_key_label = f"{selected_provider.get_display_name()} API Key"
    api_key_help = f"{selected_provider.get_display_name()} APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"

    if selected_provider == LLMProvider.OPENAI:
        api_key = st.text_input(
            api_key_label,
            value=os.getenv("OPENAI_API_KEY", ""),
            type="password",
            help=api_key_help,
        )
    elif selected_provider == LLMProvider.OPENROUTER:
        api_key = st.text_input(
            api_key_label,
            value=os.getenv("OPENROUTER_API_KEY", ""),
            type="password",
            help=api_key_help,
        )
    else:
        api_key = st.text_input(
            api_key_label,
            type="password",
            help=api_key_help,
        )

    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    available_models = LLMModel.get_models_by_provider(selected_provider)
    model_display_names = [model.get_display_name() for model in available_models]

    selected_model_display = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«é¸æŠ",
        model_display_names,
        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
    )

    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
    selected_model = None
    for model in available_models:
        if model.get_display_name() == selected_model_display:
            selected_model = model
            break

    if not api_key:
        st.warning(
            f"âš ï¸ {selected_provider.get_display_name()} API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        return

    # ãƒ‡ãƒ¼ã‚¿å…¥åŠ›è¨­å®š
    st.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")

    # å…¥åŠ›æ–¹æ³•
    input_method = st.radio(
        "ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„",
        [
            "ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœã‚’ä½¿ç”¨",
            "ãƒšãƒ¼ã‚¸ä¸Šã§ãƒˆãƒ”ãƒƒã‚¯ã‚’å®šç¾©",
            "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹",
        ],
    )

    topics_data = None
    df = None
    text_column = None

    if input_method == "ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœã‚’ä½¿ç”¨":
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‚’å–å¾—
        if "topics_result" in st.session_state:
            topics_result = st.session_state["topics_result"]

            topics_data = topics_result.dict()
            st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

            # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º
            with st.expander("ğŸ“‹ èª­ã¿è¾¼ã‚“ã ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§"):
                for topic in topics_data["topics"]:
                    st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                    for subtopic in topic.get("subtopics", []):
                        st.write(f"  - {subtopic['name']}")

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
            uploaded_file = st.file_uploader(
                "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
                type=["csv"],
                key="classification_csv",
            )

            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file)
                st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

                # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
                recommended_column, analysis = estimate_text_column(df)

                # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
                if recommended_column:
                    st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

                    with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                        recommendations = get_text_column_recommendations(df, top_n=3)
                        for i, rec in enumerate(recommendations):
                            col_name = rec["column"]
                            details = rec["details"]
                            st.write(
                                f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})"
                            )
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric(
                                    "æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}"
                                )
                            with col2:
                                st.metric(
                                    "ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}"
                                )
                            with col3:
                                st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                            st.divider()

                # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
                default_index = 0
                if recommended_column and recommended_column in df.columns:
                    default_index = df.columns.tolist().index(recommended_column)

                text_column = st.selectbox(
                    "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
                    options=df.columns.tolist(),
                    index=default_index,
                )
        else:
            st.warning("âš ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœãŒã‚ã‚Šã¾ã›ã‚“")

    elif input_method == "ãƒšãƒ¼ã‚¸ä¸Šã§ãƒˆãƒ”ãƒƒã‚¯ã‚’å®šç¾©":
        # ãƒšãƒ¼ã‚¸ä¸Šã§ã®ãƒˆãƒ”ãƒƒã‚¯å®šç¾©
        st.subheader("ğŸ“ ãƒˆãƒ”ãƒƒã‚¯å®šç¾©")

        # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        use_subtopics = st.checkbox(
            "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã‚’å«ã‚ã‚‹",
            value=False,
            help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚‚å®šç¾©ã—ã¦åˆ†é¡ã§ãã¾ã™",
        )

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if "custom_topics" not in st.session_state:
            st.session_state["custom_topics"] = [
                {
                    "id": 1,
                    "name": "",
                    "description": "",
                    "keywords": "",
                    "subtopics": [],
                }
            ]

        st.markdown("#### ãƒˆãƒ”ãƒƒã‚¯è¿½åŠ ãƒ»ç·¨é›†")

        # ãƒˆãƒ”ãƒƒã‚¯è¿½åŠ ãƒœã‚¿ãƒ³
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.button("â• ãƒˆãƒ”ãƒƒã‚¯è¿½åŠ "):
                new_id = (
                    max([t["id"] for t in st.session_state["custom_topics"]], default=0)
                    + 1
                )
                st.session_state["custom_topics"].append(
                    {
                        "id": new_id,
                        "name": "",
                        "description": "",
                        "keywords": "",
                        "subtopics": [],
                    }
                )
                st.rerun()

        with col2:
            if st.button("ğŸ—‘ï¸ å…¨ã¦ãƒªã‚»ãƒƒãƒˆ"):
                st.session_state["custom_topics"] = [
                    {
                        "id": 1,
                        "name": "",
                        "description": "",
                        "keywords": "",
                        "subtopics": [],
                    }
                ]
                st.rerun()

        # å„ãƒˆãƒ”ãƒƒã‚¯ã®ç·¨é›†
        topics_to_remove = []

        for i, topic in enumerate(st.session_state["custom_topics"]):
            with st.expander(
                f"ãƒˆãƒ”ãƒƒã‚¯ {topic['id']}: {topic['name'] or 'ï¼ˆæœªè¨­å®šï¼‰'}",
                expanded=True,
            ):
                col1, col2 = st.columns([4, 1])

                with col1:
                    # ãƒˆãƒ”ãƒƒã‚¯åŸºæœ¬æƒ…å ±
                    topic["name"] = st.text_input(
                        "ãƒˆãƒ”ãƒƒã‚¯å",
                        value=topic["name"],
                        key=f"topic_name_{topic['id']}",
                        placeholder="ä¾‹: å•†å“ã®å“è³ª",
                    )

                    topic["description"] = st.text_area(
                        "èª¬æ˜",
                        value=topic["description"],
                        key=f"topic_desc_{topic['id']}",
                        placeholder="ä¾‹: å•†å“ã®å“è³ªã«é–¢ã™ã‚‹æ„è¦‹ã‚„è©•ä¾¡",
                        height=80,
                    )

                    topic["keywords"] = st.text_input(
                        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰",
                        value=topic["keywords"],
                        key=f"topic_keywords_{topic['id']}",
                        placeholder="ä¾‹: å“è³ª, è³ª, è‰¯ã„, æ‚ªã„, è€ä¹…æ€§",
                    )

                with col2:
                    if len(st.session_state["custom_topics"]) > 1:
                        if st.button(
                            "ğŸ—‘ï¸",
                            key=f"remove_topic_{topic['id']}",
                            help="ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’å‰Šé™¤",
                        ):
                            topics_to_remove.append(i)

                # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯å®šç¾©
                if use_subtopics:
                    st.markdown("**ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯**")

                    # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯è¿½åŠ ãƒœã‚¿ãƒ³
                    if st.button(
                        "â• ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯è¿½åŠ ", key=f"add_subtopic_{topic['id']}"
                    ):
                        new_subtopic_id = (
                            max([s["id"] for s in topic["subtopics"]], default=0) + 1
                        )
                        topic["subtopics"].append(
                            {
                                "id": new_subtopic_id,
                                "name": "",
                                "description": "",
                                "keywords": "",
                            }
                        )
                        st.rerun()

                    # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ç·¨é›†
                    subtopics_to_remove = []
                    for j, subtopic in enumerate(topic["subtopics"]):
                        st.markdown(f"**ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ {subtopic['id']}**")

                        col_sub1, col_sub2 = st.columns([4, 1])

                        with col_sub1:
                            subtopic["name"] = st.text_input(
                                "åå‰",
                                value=subtopic["name"],
                                key=f"subtopic_name_{topic['id']}_{subtopic['id']}",
                                placeholder="ä¾‹: è€ä¹…æ€§",
                                label_visibility="collapsed",
                            )

                            subtopic["description"] = st.text_area(
                                "èª¬æ˜",
                                value=subtopic["description"],
                                key=f"subtopic_desc_{topic['id']}_{subtopic['id']}",
                                placeholder="ä¾‹: å•†å“ã®è€ä¹…æ€§ã«é–¢ã™ã‚‹è©•ä¾¡",
                                height=60,
                                label_visibility="collapsed",
                            )

                            subtopic["keywords"] = st.text_input(
                                "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                                value=subtopic["keywords"],
                                key=f"subtopic_keywords_{topic['id']}_{subtopic['id']}",
                                placeholder="ä¾‹: è€ä¹…æ€§, é•·æŒã¡, å£Šã‚Œã‚„ã™ã„",
                                label_visibility="collapsed",
                            )

                        with col_sub2:
                            if st.button(
                                "ğŸ—‘ï¸",
                                key=f"remove_subtopic_{topic['id']}_{subtopic['id']}",
                                help="ã“ã®ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’å‰Šé™¤",
                            ):
                                subtopics_to_remove.append(j)

                    # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯å‰Šé™¤å‡¦ç†
                    for idx in reversed(subtopics_to_remove):
                        topic["subtopics"].pop(idx)

                    if subtopics_to_remove:
                        st.rerun()

        # ãƒˆãƒ”ãƒƒã‚¯å‰Šé™¤å‡¦ç†
        for idx in reversed(topics_to_remove):
            st.session_state["custom_topics"].pop(idx)

        if topics_to_remove:
            st.rerun()

        # ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã®æ¤œè¨¼ã¨æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        valid_topics = []
        for topic in st.session_state["custom_topics"]:
            if topic["name"].strip():
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
                keywords = [
                    kw.strip() for kw in topic["keywords"].split(",") if kw.strip()
                ]

                valid_topic = {
                    "id": topic["id"],
                    "name": topic["name"].strip(),
                    "description": topic["description"].strip()
                    or topic["name"].strip(),
                    "keywords": keywords,
                }

                if use_subtopics:
                    valid_subtopics = []
                    for subtopic in topic["subtopics"]:
                        if subtopic["name"].strip():
                            sub_keywords = [
                                kw.strip()
                                for kw in subtopic["keywords"].split(",")
                                if kw.strip()
                            ]
                            valid_subtopics.append(
                                {
                                    "id": subtopic["id"],
                                    "name": subtopic["name"].strip(),
                                    "description": subtopic["description"].strip()
                                    or subtopic["name"].strip(),
                                    "keywords": sub_keywords,
                                }
                            )
                    valid_topic["subtopics"] = valid_subtopics
                else:
                    valid_topic["subtopics"] = []

                valid_topics.append(valid_topic)

        if valid_topics:
            topics_data = {"topics": valid_topics}

            # å®šç¾©ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            with st.expander("ğŸ“‹ å®šç¾©ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§"):
                for topic in valid_topics:
                    st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                    st.write(f"ã€€èª¬æ˜: {topic['description']}")
                    if topic["keywords"]:
                        st.write(f"ã€€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(topic['keywords'])}")

                    if use_subtopics and topic.get("subtopics"):
                        st.write("ã€€ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯:")
                        for subtopic in topic["subtopics"]:
                            st.write(
                                f"ã€€ã€€- {subtopic['name']}: {subtopic['description']}"
                            )
                            if subtopic["keywords"]:
                                st.write(
                                    f"ã€€ã€€ã€€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(subtopic['keywords'])}"
                                )
                    st.divider()

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
        uploaded_file = st.file_uploader(
            "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=["csv"],
            key="classification_csv_custom",
        )

        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
            recommended_column, analysis = estimate_text_column(df)

            # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
            if recommended_column:
                st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

                with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                    recommendations = get_text_column_recommendations(df, top_n=3)
                    for i, rec in enumerate(recommendations):
                        col_name = rec["column"]
                        details = rec["details"]
                        st.write(
                            f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})"
                        )
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}")
                        with col2:
                            st.metric(
                                "ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}"
                            )
                        with col3:
                            st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                        st.divider()

            # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
            default_index = 0
            if recommended_column and recommended_column in df.columns:
                default_index = df.columns.tolist().index(recommended_column)

            text_column = st.selectbox(
                "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
                options=df.columns.tolist(),
                index=default_index,
            )

    elif input_method == "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        st.subheader("ãƒˆãƒ”ãƒƒã‚¯å®šç¾©JSONãƒ•ã‚¡ã‚¤ãƒ«")
        json_file = st.file_uploader(
            "ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’å«ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=["json"],
            key="topics_json",
        )

        if json_file is not None:
            try:
                topics_data = json.load(json_file)
                if "topics" in topics_data:
                    topics_data = topics_data["topics"]
                st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

                # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º
                with st.expander("ğŸ“‹ èª­ã¿è¾¼ã‚“ã ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§"):
                    if isinstance(topics_data, dict) and "topics" in topics_data:
                        for topic in topics_data["topics"]:
                            st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                            for subtopic in topic.get("subtopics", []):
                                st.write(f"{subtopic['name']}")
                    elif isinstance(topics_data, list):
                        for topic in topics_data:
                            st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                            for subtopic in topic.get("subtopics", []):
                                st.write(f"  - {subtopic['name']}")

            except Exception as e:
                st.error(f"âš ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
        uploaded_file = st.file_uploader(
            "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=["csv"],
            key="classification_csv_json",
        )

        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
            recommended_column, analysis = estimate_text_column(df)

            # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
            if recommended_column:
                st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

                with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                    recommendations = get_text_column_recommendations(df, top_n=3)
                    for i, rec in enumerate(recommendations):
                        col_name = rec["column"]
                        details = rec["details"]
                        st.write(
                            f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})"
                        )
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}")
                        with col2:
                            st.metric(
                                "ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}"
                            )
                        with col3:
                            st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                        st.divider()

            # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
            default_index = 0
            if recommended_column and recommended_column in df.columns:
                default_index = df.columns.tolist().index(recommended_column)

            text_column = st.selectbox(
                "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
                options=df.columns.tolist(),
                index=default_index,
            )

    else:  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ãƒˆãƒ”ãƒƒã‚¯å®šç¾©JSONãƒ•ã‚¡ã‚¤ãƒ«")
            json_file = st.file_uploader(
                "ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’å«ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«", type=["json"], key="topics_json_combo"
            )

            if json_file is not None:
                try:
                    topics_data = json.load(json_file)
                    st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"âš ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

        with col2:
            st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
            uploaded_file = st.file_uploader(
                "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«",
                type=["csv"],
                key="classification_csv_combo",
            )

            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file)
                st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

                # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
                recommended_column, analysis = estimate_text_column(df)

                # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
                if recommended_column:
                    st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

                    with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                        recommendations = get_text_column_recommendations(df, top_n=3)
                        for i, rec in enumerate(recommendations):
                            col_name = rec["column"]
                            details = rec["details"]
                            st.write(
                                f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})"
                            )
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric(
                                    "æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}"
                                )
                            with col2:
                                st.metric(
                                    "ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}"
                                )
                            with col3:
                                st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                            st.divider()

                # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
                default_index = 0
                if recommended_column and recommended_column in df.columns:
                    default_index = df.columns.tolist().index(recommended_column)

                text_column = st.selectbox(
                    "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
                    options=df.columns.tolist(),
                    index=default_index,
                )

    # åˆ†é¡å‡¦ç†
    if topics_data is not None and df is not None and text_column is not None:
        st.header("ğŸ“Š åˆ†é¡")

        # ãƒ‡ãƒ¼ã‚¿èª¬æ˜å…¥åŠ›
        st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿èª¬æ˜")
        data_description = st.text_area(
            "ãƒ‡ãƒ¼ã‚¿ã®èª¬æ˜ï¼ˆä»»æ„ï¼‰",
            value="",
            placeholder="ä¾‹: ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå•†å“ã«å¯¾ã™ã‚‹æ„Ÿæƒ³ã‚„è©•ä¾¡ã‚’è¨˜è¼‰ã—ã¦ã„ã¾ã™ã€‚",
            help="ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚„èƒŒæ™¯ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã‚ˆã‚Šæ­£ç¢ºãªåˆ†é¡ã«å½¹ç«‹ã¡ã¾ã™ã€‚",
            height=100,
        )

        # ãƒ‡ãƒ¼ã‚¿åˆ¶é™
        data_limit = st.slider(
            "ãƒ‡ãƒ¼ã‚¿ä»¶æ•°",
            min_value=1,
            max_value=len(df),
            value=min(100, len(df)),
            help="APIã®åˆ¶é™ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ä»¶æ•°ã®åˆ¶é™ã§ã™",
        )

        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        filtered_df = df.head(data_limit)
        filtered_texts = filtered_df[text_column].dropna().astype(str).tolist()

        st.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿: {len(filtered_texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆ")

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°äºˆæ¸¬
        total_chars = sum(len(text) for text in filtered_texts)
        estimated_tokens = total_chars // 3
        st.warning(
            f"ğŸ“Š äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {estimated_tokens:,} tokensï¼ˆAPIã®åˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼‰"
        )

        # ä¸¦åˆ—å‡¦ç†è¨­å®š
        st.subheader("âš™ï¸ ä¸¦åˆ—å‡¦ç†è¨­å®š")
        col1, col2 = st.columns(2)

        with col1:
            batch_size = st.slider(
                "ãƒãƒƒãƒã‚µã‚¤ã‚º",
                min_value=5,
                max_value=50,
                value=10,
                step=5,
                help="ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆæ•°ï¼ˆå°ã•ã„ã»ã©å®‰å®šã€å¤§ãã„ã»ã©é«˜é€Ÿï¼‰",
            )

        with col2:
            max_workers = st.slider(
                "ä¸¦åˆ—å‡¦ç†æ•°",
                min_value=1,
                max_value=20,
                value=10,
                help="åŒæ™‚ã«å®Ÿè¡Œã™ã‚‹ãƒãƒƒãƒæ•°ï¼ˆå¤šã™ãã‚‹ã¨APIåˆ¶é™ã«æ³¨æ„ï¼‰",
            )

        # å‡¦ç†äºˆæ¸¬æƒ…å ±
        total_batches = (len(filtered_texts) + batch_size - 1) // batch_size
        st.info(
            f"ğŸ“Š å‡¦ç†äºˆæ¸¬: {total_batches}ãƒãƒƒãƒï¼ˆ{batch_size}ä»¶ãšã¤ï¼‰ã‚’{max_workers}ä¸¦åˆ—ã§å‡¦ç†"
        )

        # åˆ†é¡å®Ÿè¡Œ
        if st.button("ğŸš€ ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡å®Ÿè¡Œ", type="primary"):
            with st.spinner("ğŸ¤– LLMã«ã‚ˆã‚‹ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ä¸­..."):
                classifier = LLMTopicClassifier(
                    api_key, selected_model.value, batch_size, max_workers
                )

                # é€²æ—è¡¨ç¤º
                progress_bar = st.progress(0)
                progress_text = st.empty()

                def update_progress(progress, message):
                    progress_bar.progress(progress)
                    progress_text.text(message)

                # ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã®æ•´å½¢
                if isinstance(topics_data, dict) and "topics" in topics_data:
                    classification_result = classifier.classify_texts_parallel(
                        filtered_texts, topics_data, update_progress, data_description
                    )
                else:
                    classification_result = classifier.classify_texts_parallel(
                        filtered_texts,
                        {"topics": topics_data},
                        update_progress,
                        data_description,
                    )

                progress_bar.progress(100)
                progress_text.text("åˆ†é¡å®Œäº†ï¼")

                if classification_result:
                    st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ãŒå®Œäº†ã—ã¾ã—ãŸ")

                    # åˆ†é¡çµæœã®æ•´å½¢
                    classification_data = []
                    for cls in classification_result.classifications:
                        classification_data.append(
                            {
                                "text_index": cls.text_index,
                                "main_topic_id": cls.main_topic_id,
                                "main_topic_name": cls.main_topic_name,
                                "subtopic_id": cls.subtopic_id,
                                "subtopic_name": cls.subtopic_name,
                                "confidence": cls.confidence,
                                "sentiment": cls.sentiment.value,
                            }
                        )

                    classification_df = pd.DataFrame(classification_data)

                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                    st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
                    st.write(f"  - å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(filtered_df)}")
                    st.write(f"  - åˆ†é¡çµæœä»¶æ•°: {len(classification_df)}")
                    st.write(
                        f"  - åˆ†é¡çµæœã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²: {classification_df['text_index'].min()} - {classification_df['text_index'].max()}"
                    )

                    # çµæœã®çµåˆ
                    result_df = filtered_df.copy()
                    result_df = result_df.reset_index(drop=True)

                    # åˆ†é¡çµæœãŒãªã„è¡Œã®ãŸã‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                    result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ID"] = None
                    result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯"] = "æœªåˆ†é¡"
                    result_df["ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ID"] = None
                    result_df["ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"] = "æœªåˆ†é¡"
                    result_df["åˆ†é¡ç¢ºåº¦"] = 0.0
                    result_df["ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ"] = "æœªåˆ†é¡"

                    # åˆ†é¡çµæœã‚’text_indexã«åŸºã¥ã„ã¦ãƒãƒ¼ã‚¸
                    for _, row in classification_df.iterrows():
                        idx = row["text_index"]
                        if 0 <= idx < len(result_df):
                            result_df.loc[idx, "ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ID"] = row[
                                "main_topic_id"
                            ]
                            result_df.loc[idx, "ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯"] = row[
                                "main_topic_name"
                            ]
                            result_df.loc[idx, "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ID"] = row["subtopic_id"]
                            result_df.loc[idx, "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"] = row["subtopic_name"]
                            result_df.loc[idx, "åˆ†é¡ç¢ºåº¦"] = row["confidence"]
                            result_df.loc[idx, "ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ"] = row["sentiment"]

                    # åˆ†é¡çµ±è¨ˆã‚’è¡¨ç¤º
                    classified_count = len(
                        result_df[result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯"] != "æœªåˆ†é¡"]
                    )
                    st.write(f"  - åˆ†é¡æ¸ˆã¿ä»¶æ•°: {classified_count} / {len(result_df)}")

                    if classified_count < len(result_df):
                        st.warning(
                            f"âš ï¸ {len(result_df) - classified_count}ä»¶ãŒæœªåˆ†é¡ã§ã™ã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ä¸¦åˆ—å‡¦ç†æ•°ã‚’èª¿æ•´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚"
                        )

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ä¿å­˜
                    st.session_state["classification_result"] = result_df
                    st.session_state["classification_summary"] = classification_df

                    # çµæœè¡¨ç¤º
                    st.header("ğŸ“Š åˆ†é¡çµæœ")

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("åˆ†é¡ä»¶æ•°", len(result_df))
                    with col2:
                        st.metric(
                            "å¹³å‡ç¢ºåº¦", f"{classification_df['confidence'].mean():.2f}"
                        )
                    with col3:
                        unique_topics = classification_df["main_topic_name"].nunique()
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒˆãƒ”ãƒƒã‚¯æ•°", unique_topics)

                    # åˆ†é¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«
                    st.subheader("ğŸ“‹ åˆ†é¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«")
                    st.dataframe(
                        result_df[
                            ["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯", "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯", "åˆ†é¡ç¢ºåº¦", text_column]
                        ].head(10),
                        use_container_width=True,
                    )

                    progress_bar.empty()
                    progress_text.empty()

                else:
                    st.error(
                        "âš ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰"
                    )
                    progress_bar.empty()
                    progress_text.empty()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åˆ†é¡çµæœãŒã‚ã‚‹å ´åˆã®è¡¨ç¤º
    if "classification_result" in st.session_state:
        result_df = st.session_state["classification_result"]
        classification_df = st.session_state["classification_summary"]

        # ã‚°ãƒ©ãƒ•
        st.header("ğŸ“Š ã‚°ãƒ©ãƒ•")

        # åˆ†é¡çµæœã®å¯è¦–åŒ–
        fig_topic_dist, fig_subtopic_dist, fig_confidence = (
            create_classification_charts(classification_df)
        )

        col1, col2 = st.columns(2)
        with col1:
            st.plotly_chart(fig_topic_dist, use_container_width=True)
        with col2:
            st.plotly_chart(fig_confidence, use_container_width=True)

        st.plotly_chart(fig_subtopic_dist, use_container_width=True)

        # ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚
        fig_matrix = create_topic_subtopic_matrix(classification_df)
        st.plotly_chart(fig_matrix, use_container_width=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.header("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

        col1, col2 = st.columns(2)

        with col1:
            # å…¨ä½“çµæœCSV
            csv_full = result_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="ğŸ“¥ å…¨ä½“çµæœ (CSV)",
                data=csv_full,
                file_name="topic_classification_full.csv",
                mime="text/csv",
                key="download_full_csv",
            )

        with col2:
            # åˆ†é¡çµæœCSV
            csv_summary = classification_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="ğŸ“¥ åˆ†é¡çµæœ (CSV)",
                data=csv_summary,
                file_name="topic_classification_summary.csv",
                mime="text/csv",
                key="download_summary_csv",
            )

        # JSONå½¢å¼ã§å‡ºåŠ›
        classification_json = {
            "classification_settings": {
                "provider": selected_provider.get_display_name(),
                "model": selected_model.get_display_name(),
                "data_count": len(result_df),
                "unique_topics": classification_df["main_topic_name"].nunique(),
            },
            "classifications": classification_df.to_dict("records"),
        }

        json_str = json.dumps(classification_json, ensure_ascii=False, indent=2)

        st.download_button(
            label="ğŸ“¥ åˆ†é¡çµæœ (JSON)",
            data=json_str,
            file_name="topic_classification.json",
            mime="application/json",
            key="download_classification_json",
        )

    else:
        # ä½¿ç”¨æ–¹æ³•
        with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
            st.markdown("""
            ### ğŸ“– ä½¿ç”¨æ–¹æ³•
            1. **OpenAI API Key**ã®è¨­å®š
            2. **å…¥åŠ›æ–¹æ³•**ã®é¸æŠ:
               - ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‹ã‚‰åˆ†é¡
               - JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
               - CSVã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹
            3. **ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«**ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            4. **åˆ†é¡å®Ÿè¡Œ**ã®é¸æŠ
            5. **ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡çµæœ**ã®ç¢ºèª

            ### ğŸ” æ©Ÿèƒ½èª¬æ˜
            - **ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡**: LLMã«ã‚ˆã‚‹è‡ªå‹•ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡
            - **ä¿¡é ¼åº¦è¡¨ç¤º**: åˆ†é¡çµæœã®ä¿¡é ¼åº¦ã‚’è¡¨ç¤º
            - **å¯è¦–åŒ–**: çµæœã®ã‚°ãƒ©ãƒ•è¡¨ç¤º
            - **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: çµæœã‚’CSVå‡ºåŠ›

            ### ğŸ“Š å‡ºåŠ›å½¢å¼
            - **å…¨ä½“çµæœ**: CSV + ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡çµæœ
            - **åˆ†é¡çµæœ**: åˆ†é¡çµæœã®è¡¨ç¤º
            - **JSONå½¢å¼**: å…¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜

            ### âš™ï¸ è¨­å®šé …ç›®
            - **APIè¨­å®š**: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆ¶é™è¨­å®š
            - **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
            - **ãƒãƒƒãƒå‡¦ç†**: 50ä»¶ãšã¤ã®ä¸€æ‹¬å‡¦ç†
            - **ä¸¦åˆ—å‡¦ç†**: è¤‡æ•°ãƒãƒƒãƒã®åŒæ™‚å®Ÿè¡Œ

            ### ğŸ’¡ æŠ€è¡“ä»•æ§˜
            - **Structured Output**: æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›
            - **ãƒãƒƒãƒå‡¦ç†**: åŠ¹ç‡çš„ãªå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            - **ä¸¦åˆ—å‡¦ç†**: ThreadPoolExecutorã«ã‚ˆã‚‹é«˜é€ŸåŒ–
            - **å¯è¦–åŒ–**: çµæœã®ã‚°ãƒ©ãƒ•è¡¨ç¤º
            - **åˆ†é¡ç²¾åº¦**: é«˜ç²¾åº¦ãªåˆ†é¡å‡¦ç†
            - **æ‹¡å¼µæ€§**: æŸ”è»Ÿãªæ©Ÿèƒ½æ‹¡å¼µ

            ### ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„
            - **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 10-100ä»¶ã§èª¿æ•´å¯èƒ½
            - **ä¸¦åˆ—æ•°**: 1-10ä¸¦åˆ—ã§èª¿æ•´å¯èƒ½
            - **é€²æ—è¡¨ç¤º**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç¢ºèª
            - **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å€‹åˆ¥ãƒãƒƒãƒã®ã‚¨ãƒ©ãƒ¼å‡¦ç†
            """)


if __name__ == "__main__":
    main()
