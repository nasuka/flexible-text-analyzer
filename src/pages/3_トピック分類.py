import json
import os

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

from schema.llm_models import LLMModels
from services.text_column_estimator import (
    estimate_text_column,
    get_text_column_recommendations,
)
from services.topic_classifier import LLMTopicClassifier


def create_classification_charts(df_classified: pd.DataFrame) -> tuple:
    """åˆ†é¡çµæœã®å¯è¦–åŒ–"""

    # ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ
    topic_counts = df_classified["main_topic_name"].value_counts()
    fig_topic_dist = px.pie(
        values=topic_counts.values, names=topic_counts.index, title="ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ"
    )

    # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆä¸Šä½10ï¼‰
    subtopic_counts = df_classified["subtopic_name"].value_counts().head(10)
    fig_subtopic_dist = px.bar(
        x=subtopic_counts.values,
        y=subtopic_counts.index,
        orientation="h",
        title="ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆä¸Šä½10ï¼‰",
        labels={"x": "ä»¶æ•°", "y": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"},
    )

    # ä¿¡é ¼åº¦åˆ†å¸ƒ
    fig_confidence = px.histogram(
        df_classified,
        x="confidence",
        nbins=20,
        title="åˆ†é¡ä¿¡é ¼åº¦åˆ†å¸ƒ",
        labels={"x": "ä¿¡é ¼åº¦", "y": "ä»¶æ•°"},
    )

    return fig_topic_dist, fig_subtopic_dist, fig_confidence


def create_topic_subtopic_matrix(df_classified: pd.DataFrame) -> go.Figure:
    """ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–"""

    crosstab = pd.crosstab(
        df_classified["main_topic_name"], df_classified["subtopic_name"]
    )

    fig = px.imshow(
        crosstab.values,
        labels={"x": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯", "y": "ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯", "color": "ä»¶æ•°"},
        x=crosstab.columns,
        y=crosstab.index,
        title="ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚",
    )

    return fig


def main():
    st.title("ğŸ“Š LLMãƒˆãƒ”ãƒƒã‚¯åˆ†é¡")
    st.markdown("---")

    # OpenAI APIè¨­å®š
    st.header("ğŸ”‘ APIè¨­å®š")
    api_key = st.text_input(
        "OpenAI API Key",
        value=os.getenv("OPENAI_API_KEY", ""),
        type="password",
        help="OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
    )

    model = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«",
        LLMModels.get_model_names(),
        help="Structured Outputã«å¯¾å¿œã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
    )

    if not api_key:
        st.warning("âš ï¸ OpenAI API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        return

    # ãƒ‡ãƒ¼ã‚¿å…¥åŠ›è¨­å®š
    st.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")

    # å…¥åŠ›æ–¹æ³•
    input_method = st.radio(
        "ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„",
        [
            "ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœã‚’ä½¿ç”¨",
            "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹",
        ],
    )

    topics_data = None
    df = None
    text_column = None

    if input_method == "ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœã‚’ä½¿ç”¨":
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‚’å–å¾—
        if "topics_result" in st.session_state:
            topics_result = st.session_state["topics_result"]

            topics_data = topics_result.dict()
            st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

            # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º
            with st.expander("ğŸ“‹ èª­ã¿è¾¼ã‚“ã ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§"):
                for topic in topics_data["topics"]:
                    st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                    for subtopic in topic.get("subtopics", []):
                        st.write(f"  - {subtopic['name']}")

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
            uploaded_file = st.file_uploader(
                "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
                type=["csv"],
                key="classification_csv",
            )

            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file)
                st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

                # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
                recommended_column, analysis = estimate_text_column(df)

                # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
                if recommended_column:
                    st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

                    with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                        recommendations = get_text_column_recommendations(df, top_n=3)
                        for i, rec in enumerate(recommendations):
                            col_name = rec["column"]
                            details = rec["details"]
                            st.write(
                                f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})"
                            )
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric(
                                    "æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}"
                                )
                            with col2:
                                st.metric(
                                    "ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}"
                                )
                            with col3:
                                st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                            st.divider()

                # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
                default_index = 0
                if recommended_column and recommended_column in df.columns:
                    default_index = df.columns.tolist().index(recommended_column)

                text_column = st.selectbox(
                    "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
                    options=df.columns.tolist(),
                    index=default_index,
                )
        else:
            st.warning("âš ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœãŒã‚ã‚Šã¾ã›ã‚“")

    elif input_method == "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        st.subheader("ãƒˆãƒ”ãƒƒã‚¯å®šç¾©JSONãƒ•ã‚¡ã‚¤ãƒ«")
        json_file = st.file_uploader(
            "ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’å«ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=["json"],
            key="topics_json",
        )

        if json_file is not None:
            try:
                topics_data = json.load(json_file)
                if "topics" in topics_data:
                    topics_data = topics_data["topics"]
                st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

                # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º
                with st.expander("ğŸ“‹ èª­ã¿è¾¼ã‚“ã ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§"):
                    if isinstance(topics_data, dict) and "topics" in topics_data:
                        for topic in topics_data["topics"]:
                            st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                            for subtopic in topic.get("subtopics", []):
                                st.write(f"{subtopic['name']}")
                    elif isinstance(topics_data, list):
                        for topic in topics_data:
                            st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                            for subtopic in topic.get("subtopics", []):
                                st.write(f"  - {subtopic['name']}")

            except Exception as e:
                st.error(f"âš ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
        uploaded_file = st.file_uploader(
            "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=["csv"],
            key="classification_csv_json",
        )

        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
            recommended_column, analysis = estimate_text_column(df)

            # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
            if recommended_column:
                st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

                with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                    recommendations = get_text_column_recommendations(df, top_n=3)
                    for i, rec in enumerate(recommendations):
                        col_name = rec["column"]
                        details = rec["details"]
                        st.write(
                            f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})"
                        )
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}")
                        with col2:
                            st.metric(
                                "ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}"
                            )
                        with col3:
                            st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                        st.divider()

            # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
            default_index = 0
            if recommended_column and recommended_column in df.columns:
                default_index = df.columns.tolist().index(recommended_column)

            text_column = st.selectbox(
                "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
                options=df.columns.tolist(),
                index=default_index,
            )

    else:  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ãƒˆãƒ”ãƒƒã‚¯å®šç¾©JSONãƒ•ã‚¡ã‚¤ãƒ«")
            json_file = st.file_uploader(
                "ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’å«ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«", type=["json"], key="topics_json_combo"
            )

            if json_file is not None:
                try:
                    topics_data = json.load(json_file)
                    st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"âš ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

        with col2:
            st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
            uploaded_file = st.file_uploader(
                "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«",
                type=["csv"],
                key="classification_csv_combo",
            )

            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file)
                st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

                # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ æ¨å®š
                recommended_column, analysis = estimate_text_column(df)

                # æ¨å¥¨ã‚«ãƒ©ãƒ è¡¨ç¤º
                if recommended_column:
                    st.success(f"ğŸ’¡ æ¨å¥¨ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ : **{recommended_column}**")

                    with st.expander("ğŸ“Š ã‚«ãƒ©ãƒ åˆ†æè©³ç´°"):
                        recommendations = get_text_column_recommendations(df, top_n=3)
                        for i, rec in enumerate(recommendations):
                            col_name = rec["column"]
                            details = rec["details"]
                            st.write(
                                f"**{i + 1}ä½: {col_name}** (ã‚¹ã‚³ã‚¢: {rec['score']:.1f})"
                            )
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric(
                                    "æ—¥æœ¬èªç‡", f"{details['japanese_ratio']:.1%}"
                                )
                            with col2:
                                st.metric(
                                    "ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‡", f"{details['uniqueness_ratio']:.1%}"
                                )
                            with col3:
                                st.metric("å¹³å‡æ–‡å­—æ•°", f"{details['avg_length']:.0f}")
                            st.divider()

                # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠï¼ˆæ¨å¥¨ã‚«ãƒ©ãƒ ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
                default_index = 0
                if recommended_column and recommended_column in df.columns:
                    default_index = df.columns.tolist().index(recommended_column)

                text_column = st.selectbox(
                    "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„",
                    options=df.columns.tolist(),
                    index=default_index,
                )

    # åˆ†é¡å‡¦ç†
    if topics_data is not None and df is not None and text_column is not None:
        st.header("ğŸ“Š åˆ†é¡")

        # ãƒ‡ãƒ¼ã‚¿åˆ¶é™
        data_limit = st.slider(
            "ãƒ‡ãƒ¼ã‚¿ä»¶æ•°",
            min_value=1,
            max_value=len(df),
            value=min(100, len(df)),
            help="APIã®åˆ¶é™ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ä»¶æ•°ã®åˆ¶é™ã§ã™",
        )

        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        filtered_df = df.head(data_limit)
        filtered_texts = filtered_df[text_column].dropna().astype(str).tolist()

        st.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿: {len(filtered_texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆ")

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°äºˆæ¸¬
        total_chars = sum(len(text) for text in filtered_texts)
        estimated_tokens = total_chars // 3
        st.warning(
            f"ğŸ“Š äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {estimated_tokens:,} tokensï¼ˆAPIã®åˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼‰"
        )

        # ä¸¦åˆ—å‡¦ç†è¨­å®š
        st.subheader("âš™ï¸ ä¸¦åˆ—å‡¦ç†è¨­å®š")
        col1, col2 = st.columns(2)

        with col1:
            batch_size = st.slider(
                "ãƒãƒƒãƒã‚µã‚¤ã‚º",
                min_value=5,
                max_value=50,
                value=10,
                step=5,
                help="ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆæ•°ï¼ˆå°ã•ã„ã»ã©å®‰å®šã€å¤§ãã„ã»ã©é«˜é€Ÿï¼‰",
            )

        with col2:
            max_workers = st.slider(
                "ä¸¦åˆ—å‡¦ç†æ•°",
                min_value=1,
                max_value=20,
                value=10,
                help="åŒæ™‚ã«å®Ÿè¡Œã™ã‚‹ãƒãƒƒãƒæ•°ï¼ˆå¤šã™ãã‚‹ã¨APIåˆ¶é™ã«æ³¨æ„ï¼‰",
            )

        # å‡¦ç†äºˆæ¸¬æƒ…å ±
        total_batches = (len(filtered_texts) + batch_size - 1) // batch_size
        st.info(
            f"ğŸ“Š å‡¦ç†äºˆæ¸¬: {total_batches}ãƒãƒƒãƒï¼ˆ{batch_size}ä»¶ãšã¤ï¼‰ã‚’{max_workers}ä¸¦åˆ—ã§å‡¦ç†"
        )

        # åˆ†é¡å®Ÿè¡Œ
        if st.button("ğŸš€ ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡å®Ÿè¡Œ", type="primary"):
            with st.spinner("ğŸ¤– LLMã«ã‚ˆã‚‹ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ä¸­..."):
                classifier = LLMTopicClassifier(api_key, model, batch_size, max_workers)

                # é€²æ—è¡¨ç¤º
                progress_bar = st.progress(0)
                progress_text = st.empty()

                def update_progress(progress, message):
                    progress_bar.progress(progress)
                    progress_text.text(message)

                # ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã®æ•´å½¢
                if isinstance(topics_data, dict) and "topics" in topics_data:
                    classification_result = classifier.classify_texts_parallel(
                        filtered_texts, topics_data, update_progress
                    )
                else:
                    classification_result = classifier.classify_texts_parallel(
                        filtered_texts, {"topics": topics_data}, update_progress
                    )

                progress_bar.progress(100)
                progress_text.text("åˆ†é¡å®Œäº†ï¼")

                if classification_result:
                    st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ãŒå®Œäº†ã—ã¾ã—ãŸ")

                    # åˆ†é¡çµæœã®æ•´å½¢
                    classification_data = []
                    for cls in classification_result.classifications:
                        classification_data.append(
                            {
                                "text_index": cls.text_index,
                                "main_topic_id": cls.main_topic_id,
                                "main_topic_name": cls.main_topic_name,
                                "subtopic_id": cls.subtopic_id,
                                "subtopic_name": cls.subtopic_name,
                                "confidence": cls.confidence,
                                "sentiment": cls.sentiment.value,
                            }
                        )

                    classification_df = pd.DataFrame(classification_data)

                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                    st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
                    st.write(f"  - å…ƒãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(filtered_df)}")
                    st.write(f"  - åˆ†é¡çµæœä»¶æ•°: {len(classification_df)}")
                    st.write(
                        f"  - åˆ†é¡çµæœã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²: {classification_df['text_index'].min()} - {classification_df['text_index'].max()}"
                    )

                    # çµæœã®çµåˆ
                    result_df = filtered_df.copy()
                    result_df = result_df.reset_index(drop=True)

                    # åˆ†é¡çµæœãŒãªã„è¡Œã®ãŸã‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                    result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ID"] = None
                    result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯"] = "æœªåˆ†é¡"
                    result_df["ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ID"] = None
                    result_df["ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"] = "æœªåˆ†é¡"
                    result_df["åˆ†é¡ç¢ºåº¦"] = 0.0
                    result_df["ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ"] = "æœªåˆ†é¡"

                    # åˆ†é¡çµæœã‚’text_indexã«åŸºã¥ã„ã¦ãƒãƒ¼ã‚¸
                    for _, row in classification_df.iterrows():
                        idx = row["text_index"]
                        if 0 <= idx < len(result_df):
                            result_df.loc[idx, "ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ID"] = row[
                                "main_topic_id"
                            ]
                            result_df.loc[idx, "ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯"] = row[
                                "main_topic_name"
                            ]
                            result_df.loc[idx, "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ID"] = row["subtopic_id"]
                            result_df.loc[idx, "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"] = row["subtopic_name"]
                            result_df.loc[idx, "åˆ†é¡ç¢ºåº¦"] = row["confidence"]
                            result_df.loc[idx, "ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ"] = row["sentiment"]

                    # åˆ†é¡çµ±è¨ˆã‚’è¡¨ç¤º
                    classified_count = len(
                        result_df[result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯"] != "æœªåˆ†é¡"]
                    )
                    st.write(f"  - åˆ†é¡æ¸ˆã¿ä»¶æ•°: {classified_count} / {len(result_df)}")

                    if classified_count < len(result_df):
                        st.warning(
                            f"âš ï¸ {len(result_df) - classified_count}ä»¶ãŒæœªåˆ†é¡ã§ã™ã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ä¸¦åˆ—å‡¦ç†æ•°ã‚’èª¿æ•´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚"
                        )

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ä¿å­˜
                    st.session_state["classification_result"] = result_df
                    st.session_state["classification_summary"] = classification_df

                    # çµæœè¡¨ç¤º
                    st.header("ğŸ“Š åˆ†é¡çµæœ")

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("åˆ†é¡ä»¶æ•°", len(result_df))
                    with col2:
                        st.metric(
                            "å¹³å‡ç¢ºåº¦", f"{classification_df['confidence'].mean():.2f}"
                        )
                    with col3:
                        unique_topics = classification_df["main_topic_name"].nunique()
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒˆãƒ”ãƒƒã‚¯æ•°", unique_topics)

                    # åˆ†é¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«
                    st.subheader("ğŸ“‹ åˆ†é¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«")
                    st.dataframe(
                        result_df[
                            ["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯", "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯", "åˆ†é¡ç¢ºåº¦", text_column]
                        ].head(10),
                        use_container_width=True,
                    )

                    progress_bar.empty()
                    progress_text.empty()

                else:
                    st.error(
                        "âš ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰"
                    )
                    progress_bar.empty()
                    progress_text.empty()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åˆ†é¡çµæœãŒã‚ã‚‹å ´åˆã®è¡¨ç¤º
    if "classification_result" in st.session_state:
        result_df = st.session_state["classification_result"]
        classification_df = st.session_state["classification_summary"]

        # ã‚°ãƒ©ãƒ•
        st.header("ğŸ“Š ã‚°ãƒ©ãƒ•")

        # åˆ†é¡çµæœã®å¯è¦–åŒ–
        fig_topic_dist, fig_subtopic_dist, fig_confidence = (
            create_classification_charts(classification_df)
        )

        col1, col2 = st.columns(2)
        with col1:
            st.plotly_chart(fig_topic_dist, use_container_width=True)
        with col2:
            st.plotly_chart(fig_confidence, use_container_width=True)

        st.plotly_chart(fig_subtopic_dist, use_container_width=True)

        # ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚
        fig_matrix = create_topic_subtopic_matrix(classification_df)
        st.plotly_chart(fig_matrix, use_container_width=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.header("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

        col1, col2 = st.columns(2)

        with col1:
            # å…¨ä½“çµæœCSV
            csv_full = result_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="ğŸ“¥ å…¨ä½“çµæœ (CSV)",
                data=csv_full,
                file_name="topic_classification_full.csv",
                mime="text/csv",
                key="download_full_csv",
            )

        with col2:
            # åˆ†é¡çµæœCSV
            csv_summary = classification_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="ğŸ“¥ åˆ†é¡çµæœ (CSV)",
                data=csv_summary,
                file_name="topic_classification_summary.csv",
                mime="text/csv",
                key="download_summary_csv",
            )

        # JSONå½¢å¼ã§å‡ºåŠ›
        classification_json = {
            "classification_settings": {
                "model": model,
                "data_count": len(result_df),
                "unique_topics": classification_df["main_topic_name"].nunique(),
            },
            "classifications": classification_df.to_dict("records"),
        }

        json_str = json.dumps(classification_json, ensure_ascii=False, indent=2)

        st.download_button(
            label="ğŸ“¥ åˆ†é¡çµæœ (JSON)",
            data=json_str,
            file_name="topic_classification.json",
            mime="application/json",
            key="download_classification_json",
        )

    else:
        # ä½¿ç”¨æ–¹æ³•
        with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
            st.markdown("""
            ### ğŸ“– ä½¿ç”¨æ–¹æ³•
            1. **OpenAI API Key**ã®è¨­å®š
            2. **å…¥åŠ›æ–¹æ³•**ã®é¸æŠ:
               - ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‹ã‚‰åˆ†é¡
               - JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
               - CSVã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹
            3. **ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«**ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            4. **åˆ†é¡å®Ÿè¡Œ**ã®é¸æŠ
            5. **ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡çµæœ**ã®ç¢ºèª

            ### ğŸ” æ©Ÿèƒ½èª¬æ˜
            - **ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡**: LLMã«ã‚ˆã‚‹è‡ªå‹•ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡
            - **ä¿¡é ¼åº¦è¡¨ç¤º**: åˆ†é¡çµæœã®ä¿¡é ¼åº¦ã‚’è¡¨ç¤º
            - **å¯è¦–åŒ–**: çµæœã®ã‚°ãƒ©ãƒ•è¡¨ç¤º
            - **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: çµæœã‚’CSVå‡ºåŠ›

            ### ğŸ“Š å‡ºåŠ›å½¢å¼
            - **å…¨ä½“çµæœ**: CSV + ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡çµæœ
            - **åˆ†é¡çµæœ**: åˆ†é¡çµæœã®è¡¨ç¤º
            - **JSONå½¢å¼**: å…¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜

            ### âš™ï¸ è¨­å®šé …ç›®
            - **APIè¨­å®š**: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆ¶é™è¨­å®š
            - **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
            - **ãƒãƒƒãƒå‡¦ç†**: 50ä»¶ãšã¤ã®ä¸€æ‹¬å‡¦ç†
            - **ä¸¦åˆ—å‡¦ç†**: è¤‡æ•°ãƒãƒƒãƒã®åŒæ™‚å®Ÿè¡Œ

            ### ğŸ’¡ æŠ€è¡“ä»•æ§˜
            - **Structured Output**: æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›
            - **ãƒãƒƒãƒå‡¦ç†**: åŠ¹ç‡çš„ãªå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            - **ä¸¦åˆ—å‡¦ç†**: ThreadPoolExecutorã«ã‚ˆã‚‹é«˜é€ŸåŒ–
            - **å¯è¦–åŒ–**: çµæœã®ã‚°ãƒ©ãƒ•è¡¨ç¤º
            - **åˆ†é¡ç²¾åº¦**: é«˜ç²¾åº¦ãªåˆ†é¡å‡¦ç†
            - **æ‹¡å¼µæ€§**: æŸ”è»Ÿãªæ©Ÿèƒ½æ‹¡å¼µ

            ### ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„
            - **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 10-100ä»¶ã§èª¿æ•´å¯èƒ½
            - **ä¸¦åˆ—æ•°**: 1-10ä¸¦åˆ—ã§èª¿æ•´å¯èƒ½
            - **é€²æ—è¡¨ç¤º**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç¢ºèª
            - **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å€‹åˆ¥ãƒãƒƒãƒã®ã‚¨ãƒ©ãƒ¼å‡¦ç†
            """)


if __name__ == "__main__":
    main()
