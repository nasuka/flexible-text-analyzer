import asyncio
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any

import openai
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from pydantic import BaseModel


class TopicClassification(BaseModel):
    """ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã®çµæœ"""

    text_index: int
    main_topic_id: int
    main_topic_name: str
    subtopic_id: int
    subtopic_name: str
    confidence: float
    reasoning: str


class ClassificationResult(BaseModel):
    """åˆ†é¡çµæœ"""

    classifications: list[TopicClassification]


class LLMTopicClassifier:
    def __init__(self, api_key: str, model: str = "gpt-4o-2024-08-06", batch_size: int = 50, max_workers: int = 5):
        """OpenAI APIã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã®Structured Outputã‚’å–å¾—"""
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.batch_size = batch_size
        self.max_workers = max_workers

    def _create_topic_definitions(self, topics_data: dict[str, Any]) -> str:
        """ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã®ä½œæˆ"""
        topic_info = []
        for topic in topics_data.get("topics", []):
            topic_str = f"ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}\n"
            topic_str += f"  èª¬æ˜: {topic['description']}\n"
            topic_str += f"  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(topic['keywords'])}\n"

            if topic.get("subtopics"):
                topic_str += "  ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯:\n"
                for subtopic in topic["subtopics"]:
                    topic_str += f"    {subtopic['id']}: {subtopic['name']}\n"
                    topic_str += f"      èª¬æ˜: {subtopic['description']}\n"
                    topic_str += (
                        f"      ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(subtopic['keywords'])}\n"
                    )

            topic_info.append(topic_str)

        return "\n\n".join(topic_info)

    def _classify_batch(
        self, batch_texts: list[str], batch_start_index: int, topic_definitions: str
    ) -> ClassificationResult | None:
        """ãƒãƒƒãƒã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡ã™ã‚‹"""
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®ä½œæˆï¼ˆãƒãƒƒãƒå†…ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ï¼‰
        text_list = "\n".join([f"{i}: {text}" for i, text in enumerate(batch_texts)])

        prompt = f"""
ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã«åŸºã¥ã„ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ãƒˆãƒ”ãƒƒã‚¯å®šç¾©:
{topic_definitions}

ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ:
{text_list}

åˆ†é¡ãƒ«ãƒ¼ãƒ«:
1. ãƒ†ã‚­ã‚¹ãƒˆã‚’0ã‹ã‚‰å§‹ã¾ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆã“ã®ãƒãƒƒãƒå†…ã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
2. ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ãã ã•ã„
3. ä¿¡é ¼åº¦ã‚’0-1ã®æ•°å€¤ã§æŒ‡å®šã—ã¦ãã ã•ã„
4. åˆ†é¡ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„
5. æœ€ã‚‚é©åˆ‡ãªãƒˆãƒ”ãƒƒã‚¯ã‚’é¸æŠã—ã¦ãã ã•ã„
"""

        try:
            response = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ”ãƒƒã‚¯ã«åˆ†é¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
                response_format=ClassificationResult,
                temperature=0.1,
            )

            result = response.choices[0].message.parsed
            
            # ãƒãƒƒãƒå†…ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¨ä½“ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«èª¿æ•´
            if result:
                for classification in result.classifications:
                    classification.text_index += batch_start_index
            
            return result

        except Exception as e:
            st.error(f"ãƒãƒƒãƒåˆ†é¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return None

    def classify_texts_parallel(
        self, texts: list[str], topics_data: dict[str, Any], progress_callback=None
    ) -> ClassificationResult | None:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸¦åˆ—ã§ãƒãƒƒãƒåˆ†é¡ã™ã‚‹"""
        
        topic_definitions = self._create_topic_definitions(topics_data)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒã«åˆ†å‰²
        batches = []
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batches.append((batch_texts, i))
        
        all_classifications = []
        completed_batches = 0
        
        # ä¸¦åˆ—å‡¦ç†ã§ãƒãƒƒãƒã‚’å‡¦ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å…¨ãƒãƒƒãƒã®ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
            future_to_batch = {
                executor.submit(self._classify_batch, batch_texts, start_index, topic_definitions): (batch_texts, start_index)
                for batch_texts, start_index in batches
            }
            
            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
            for future in as_completed(future_to_batch):
                batch_texts, start_index = future_to_batch[future]
                try:
                    result = future.result()
                    if result and result.classifications:
                        all_classifications.extend(result.classifications)
                    
                    completed_batches += 1
                    
                    # é€²æ—æ›´æ–°
                    if progress_callback:
                        progress = int((completed_batches / len(batches)) * 100)
                        progress_callback(progress, f"ãƒãƒƒãƒ {completed_batches}/{len(batches)} å®Œäº†")
                        
                except Exception as e:
                    st.error(f"ãƒãƒƒãƒå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        
        # çµæœã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«ã‚½ãƒ¼ãƒˆ
        all_classifications.sort(key=lambda x: x.text_index)
        
        return ClassificationResult(classifications=all_classifications)

    def classify_texts(
        self, texts: list[str], topics_data: dict[str, Any]
    ) -> ClassificationResult | None:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®å¾“æ¥ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆéæ¨å¥¨ï¼‰"""
        return self.classify_texts_parallel(texts, topics_data)


def create_classification_charts(df_classified: pd.DataFrame) -> tuple:
    """åˆ†é¡çµæœã®å¯è¦–åŒ–"""

    # ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ
    topic_counts = df_classified["main_topic_name"].value_counts()
    fig_topic_dist = px.pie(
        values=topic_counts.values, names=topic_counts.index, title="ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ"
    )

    # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆä¸Šä½10ï¼‰
    subtopic_counts = df_classified["subtopic_name"].value_counts().head(10)
    fig_subtopic_dist = px.bar(
        x=subtopic_counts.values,
        y=subtopic_counts.index,
        orientation="h",
        title="ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒï¼ˆä¸Šä½10ï¼‰",
        labels={"x": "ä»¶æ•°", "y": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"},
    )

    # ä¿¡é ¼åº¦åˆ†å¸ƒ
    fig_confidence = px.histogram(
        df_classified,
        x="confidence",
        nbins=20,
        title="åˆ†é¡ä¿¡é ¼åº¦åˆ†å¸ƒ",
        labels={"x": "ä¿¡é ¼åº¦", "y": "ä»¶æ•°"},
    )

    return fig_topic_dist, fig_subtopic_dist, fig_confidence


def create_topic_subtopic_matrix(df_classified: pd.DataFrame) -> go.Figure:
    """ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚ã‚’å¯è¦–åŒ–"""

    crosstab = pd.crosstab(
        df_classified["main_topic_name"], df_classified["subtopic_name"]
    )

    fig = px.imshow(
        crosstab.values,
        labels={"x": "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯", "y": "ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯", "color": "ä»¶æ•°"},
        x=crosstab.columns,
        y=crosstab.index,
        title="ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚",
    )

    return fig


def main():
    st.title("ğŸ“Š LLMãƒˆãƒ”ãƒƒã‚¯åˆ†é¡")
    st.markdown("---")

    # OpenAI APIè¨­å®š
    st.header("ğŸ”‘ APIè¨­å®š")
    api_key = st.text_input(
        "OpenAI API Key",
        value=os.getenv("OPENAI_API_KEY", ""),
        type="password",
        help="OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
    )

    model = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«",
        ["gpt-4o-2024-08-06", "gpt-4o-mini"],
        help="Structured Outputã«å¯¾å¿œã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
    )

    if not api_key:
        st.warning("âš ï¸ OpenAI API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        return

    # ãƒ‡ãƒ¼ã‚¿å…¥åŠ›è¨­å®š
    st.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")

    # å…¥åŠ›æ–¹æ³•
    input_method = st.radio(
        "ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„",
        [
            "ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœã‚’ä½¿ç”¨",
            "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹",
        ],
    )

    topics_data = None
    df = None
    text_column = None

    if input_method == "ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœã‚’ä½¿ç”¨":
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‚’å–å¾—
        if "topics_result" in st.session_state:
            topics_result = st.session_state["topics_result"]

            topics_data = topics_result.dict()
            st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

            # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º
            with st.expander("ğŸ“‹ èª­ã¿è¾¼ã‚“ã ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§"):
                for topic in topics_data["topics"]:
                    st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                    for subtopic in topic.get("subtopics", []):
                        st.write(f"  - {subtopic['name']}")

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
            uploaded_file = st.file_uploader(
                "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
                type=["csv"],
                key="classification_csv",
            )

            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file)
                st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

                # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠ
                text_column = st.selectbox(
                    "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„", options=df.columns.tolist()
                )
        else:
            st.warning("âš ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ†æã®çµæœãŒã‚ã‚Šã¾ã›ã‚“")

    elif input_method == "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        st.subheader("ãƒˆãƒ”ãƒƒã‚¯å®šç¾©JSONãƒ•ã‚¡ã‚¤ãƒ«")
        json_file = st.file_uploader(
            "ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’å«ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=["json"],
            key="topics_json",
        )

        if json_file is not None:
            try:
                topics_data = json.load(json_file)
                if "topics" in topics_data:
                    topics_data = topics_data["topics"]
                st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

                # ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§ã‚’è¡¨ç¤º
                with st.expander("ğŸ“‹ èª­ã¿è¾¼ã‚“ã ãƒˆãƒ”ãƒƒã‚¯ä¸€è¦§"):
                    if isinstance(topics_data, dict) and "topics" in topics_data:
                        for topic in topics_data["topics"]:
                            st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                            for subtopic in topic.get("subtopics", []):
                                st.write(f"{subtopic['name']}")
                    elif isinstance(topics_data, list):
                        for topic in topics_data:
                            st.write(f"**ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}**")
                            for subtopic in topic.get("subtopics", []):
                                st.write(f"  - {subtopic['name']}")

            except Exception as e:
                st.error(f"âš ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
        uploaded_file = st.file_uploader(
            "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            type=["csv"],
            key="classification_csv_json",
        )

        if uploaded_file is not None:
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

            # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠ
            text_column = st.selectbox(
                "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„", options=df.columns.tolist()
            )

    else:  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ãƒˆãƒ”ãƒƒã‚¯å®šç¾©JSONãƒ•ã‚¡ã‚¤ãƒ«")
            json_file = st.file_uploader(
                "ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’å«ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«", type=["json"], key="topics_json_combo"
            )

            if json_file is not None:
                try:
                    topics_data = json.load(json_file)
                    st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"âš ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

        with col2:
            st.subheader("ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«")
            uploaded_file = st.file_uploader(
                "ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«",
                type=["csv"],
                key="classification_csv_combo",
            )

            if uploaded_file is not None:
                df = pd.read_csv(uploaded_file)
                st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")

                # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®é¸æŠ
                text_column = st.selectbox(
                    "ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€åˆ—ã‚’é¸æŠã—ã¦ãã ã•ã„", options=df.columns.tolist()
                )

    # åˆ†é¡å‡¦ç†
    if topics_data is not None and df is not None and text_column is not None:
        st.header("ğŸ“Š åˆ†é¡")

        # ãƒ‡ãƒ¼ã‚¿åˆ¶é™
        data_limit = st.slider(
            "ãƒ‡ãƒ¼ã‚¿ä»¶æ•°",
            min_value=1,
            max_value=len(df),
            value=min(100, len(df)),
            help="APIã®åˆ¶é™ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ä»¶æ•°ã®åˆ¶é™ã§ã™",
        )

        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        filtered_df = df.head(data_limit)
        filtered_texts = filtered_df[text_column].dropna().astype(str).tolist()

        st.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿: {len(filtered_texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆ")

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°äºˆæ¸¬
        total_chars = sum(len(text) for text in filtered_texts)
        estimated_tokens = total_chars // 3
        st.warning(
            f"ğŸ“Š äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {estimated_tokens:,} tokensï¼ˆAPIã®åˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼‰"
        )

        # ä¸¦åˆ—å‡¦ç†è¨­å®š
        st.subheader("âš™ï¸ ä¸¦åˆ—å‡¦ç†è¨­å®š")
        col1, col2 = st.columns(2)
        
        with col1:
            batch_size = st.slider(
                "ãƒãƒƒãƒã‚µã‚¤ã‚º",
                min_value=10,
                max_value=100,
                value=50,
                step=10,
                help="ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆæ•°"
            )
        
        with col2:
            max_workers = st.slider(
                "ä¸¦åˆ—å‡¦ç†æ•°",
                min_value=1,
                max_value=10,
                value=5,
                help="åŒæ™‚ã«å®Ÿè¡Œã™ã‚‹ãƒãƒƒãƒæ•°"
            )

        # å‡¦ç†äºˆæ¸¬æƒ…å ±
        total_batches = (len(filtered_texts) + batch_size - 1) // batch_size
        st.info(f"ğŸ“Š å‡¦ç†äºˆæ¸¬: {total_batches}ãƒãƒƒãƒï¼ˆ{batch_size}ä»¶ãšã¤ï¼‰ã‚’{max_workers}ä¸¦åˆ—ã§å‡¦ç†")

        # åˆ†é¡å®Ÿè¡Œ
        if st.button("ğŸš€ ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡å®Ÿè¡Œ", type="primary"):
            with st.spinner("ğŸ¤– LLMã«ã‚ˆã‚‹ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ä¸­..."):
                classifier = LLMTopicClassifier(api_key, model, batch_size, max_workers)

                # é€²æ—è¡¨ç¤º
                progress_bar = st.progress(0)
                progress_text = st.empty()
                
                def update_progress(progress, message):
                    progress_bar.progress(progress)
                    progress_text.text(message)

                # ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã®æ•´å½¢
                if isinstance(topics_data, dict) and "topics" in topics_data:
                    classification_result = classifier.classify_texts_parallel(
                        filtered_texts, topics_data, update_progress
                    )
                else:
                    classification_result = classifier.classify_texts_parallel(
                        filtered_texts, {"topics": topics_data}, update_progress
                    )

                progress_bar.progress(100)
                progress_text.text("åˆ†é¡å®Œäº†ï¼")

                if classification_result:
                    st.success("âœ… ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ãŒå®Œäº†ã—ã¾ã—ãŸ")

                    # åˆ†é¡çµæœã®æ•´å½¢
                    classification_data = []
                    for cls in classification_result.classifications:
                        classification_data.append(
                            {
                                "text_index": cls.text_index,
                                "main_topic_id": cls.main_topic_id,
                                "main_topic_name": cls.main_topic_name,
                                "subtopic_id": cls.subtopic_id,
                                "subtopic_name": cls.subtopic_name,
                                "confidence": cls.confidence,
                                "reasoning": cls.reasoning,
                            }
                        )

                    classification_df = pd.DataFrame(classification_data)

                    # çµæœã®çµåˆ
                    result_df = filtered_df.copy()
                    result_df = result_df.reset_index(drop=True)

                    # åˆ†é¡çµæœã®è¿½åŠ 
                    result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ID"] = classification_df["main_topic_id"]
                    result_df["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯"] = classification_df["main_topic_name"]
                    result_df["ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ID"] = classification_df["subtopic_id"]
                    result_df["ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯"] = classification_df["subtopic_name"]
                    result_df["åˆ†é¡ç¢ºåº¦"] = classification_df["confidence"]
                    result_df["åˆ†é¡ç†ç”±"] = classification_df["reasoning"]

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®ä¿å­˜
                    st.session_state["classification_result"] = result_df
                    st.session_state["classification_summary"] = classification_df

                    # çµæœè¡¨ç¤º
                    st.header("ğŸ“Š åˆ†é¡çµæœ")

                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("åˆ†é¡ä»¶æ•°", len(result_df))
                    with col2:
                        st.metric(
                            "å¹³å‡ç¢ºåº¦", f"{classification_df['confidence'].mean():.2f}"
                        )
                    with col3:
                        unique_topics = classification_df["main_topic_name"].nunique()
                        st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒˆãƒ”ãƒƒã‚¯æ•°", unique_topics)

                    # åˆ†é¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«
                    st.subheader("ğŸ“‹ åˆ†é¡çµæœãƒ†ãƒ¼ãƒ–ãƒ«")
                    st.dataframe(
                        result_df[
                            ["ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯", "ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯", "åˆ†é¡ç¢ºåº¦", text_column]
                        ].head(10),
                        use_container_width=True,
                    )

                    progress_bar.empty()
                    progress_text.empty()

                else:
                    st.error(
                        "âš ï¸ ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIã®å¿œç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰"
                    )
                    progress_bar.empty()
                    progress_text.empty()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«åˆ†é¡çµæœãŒã‚ã‚‹å ´åˆã®è¡¨ç¤º
    if "classification_result" in st.session_state:
        result_df = st.session_state["classification_result"]
        classification_df = st.session_state["classification_summary"]

        # ã‚°ãƒ©ãƒ•
        st.header("ğŸ“Š ã‚°ãƒ©ãƒ•")

        # åˆ†é¡çµæœã®å¯è¦–åŒ–
        fig_topic_dist, fig_subtopic_dist, fig_confidence = (
            create_classification_charts(classification_df)
        )

        col1, col2 = st.columns(2)
        with col1:
            st.plotly_chart(fig_topic_dist, use_container_width=True)
        with col2:
            st.plotly_chart(fig_confidence, use_container_width=True)

        st.plotly_chart(fig_subtopic_dist, use_container_width=True)

        # ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã®é–¢ä¿‚
        fig_matrix = create_topic_subtopic_matrix(classification_df)
        st.plotly_chart(fig_matrix, use_container_width=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        st.header("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

        col1, col2 = st.columns(2)

        with col1:
            # å…¨ä½“çµæœCSV
            csv_full = result_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="ğŸ“¥ å…¨ä½“çµæœ (CSV)",
                data=csv_full,
                file_name="topic_classification_full.csv",
                mime="text/csv",
                key="download_full_csv",
            )

        with col2:
            # åˆ†é¡çµæœCSV
            csv_summary = classification_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="ğŸ“¥ åˆ†é¡çµæœ (CSV)",
                data=csv_summary,
                file_name="topic_classification_summary.csv",
                mime="text/csv",
                key="download_summary_csv",
            )

        # JSONå½¢å¼ã§å‡ºåŠ›
        classification_json = {
            "classification_settings": {
                "model": model,
                "data_count": len(result_df),
                "unique_topics": classification_df["main_topic_name"].nunique(),
            },
            "classifications": classification_df.to_dict("records"),
        }

        json_str = json.dumps(classification_json, ensure_ascii=False, indent=2)

        st.download_button(
            label="ğŸ“¥ åˆ†é¡çµæœ (JSON)",
            data=json_str,
            file_name="topic_classification.json",
            mime="application/json",
            key="download_classification_json",
        )

    else:
        # ä½¿ç”¨æ–¹æ³•
        with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•"):
            st.markdown("""
            ### ğŸ“– ä½¿ç”¨æ–¹æ³•
            1. **OpenAI API Key**ã®è¨­å®š
            2. **å…¥åŠ›æ–¹æ³•**ã®é¸æŠ:
               - ãƒˆãƒ”ãƒƒã‚¯åˆ†æçµæœã‹ã‚‰åˆ†é¡
               - JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
               - CSVã¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¡æ–¹
            3. **ãƒ†ã‚­ã‚¹ãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«**ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            4. **åˆ†é¡å®Ÿè¡Œ**ã®é¸æŠ
            5. **ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡çµæœ**ã®ç¢ºèª

            ### ğŸ” æ©Ÿèƒ½èª¬æ˜
            - **ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡**: LLMã«ã‚ˆã‚‹è‡ªå‹•ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡
            - **ä¿¡é ¼åº¦è¡¨ç¤º**: åˆ†é¡çµæœã®ä¿¡é ¼åº¦ã‚’è¡¨ç¤º
            - **å¯è¦–åŒ–**: çµæœã®ã‚°ãƒ©ãƒ•è¡¨ç¤º
            - **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: çµæœã‚’CSVå‡ºåŠ›

            ### ğŸ“Š å‡ºåŠ›å½¢å¼
            - **å…¨ä½“çµæœ**: CSV + ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡çµæœ
            - **åˆ†é¡çµæœ**: åˆ†é¡çµæœã®è¡¨ç¤º
            - **JSONå½¢å¼**: å…¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜

            ### âš™ï¸ è¨­å®šé …ç›®
            - **APIè¨­å®š**: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆ¶é™è¨­å®š
            - **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
            - **ãƒãƒƒãƒå‡¦ç†**: 50ä»¶ãšã¤ã®ä¸€æ‹¬å‡¦ç†
            - **ä¸¦åˆ—å‡¦ç†**: è¤‡æ•°ãƒãƒƒãƒã®åŒæ™‚å®Ÿè¡Œ

            ### ğŸ’¡ æŠ€è¡“ä»•æ§˜
            - **Structured Output**: æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›
            - **ãƒãƒƒãƒå‡¦ç†**: åŠ¹ç‡çš„ãªå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            - **ä¸¦åˆ—å‡¦ç†**: ThreadPoolExecutorã«ã‚ˆã‚‹é«˜é€ŸåŒ–
            - **å¯è¦–åŒ–**: çµæœã®ã‚°ãƒ©ãƒ•è¡¨ç¤º
            - **åˆ†é¡ç²¾åº¦**: é«˜ç²¾åº¦ãªåˆ†é¡å‡¦ç†
            - **æ‹¡å¼µæ€§**: æŸ”è»Ÿãªæ©Ÿèƒ½æ‹¡å¼µ

            ### ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„
            - **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 10-100ä»¶ã§èª¿æ•´å¯èƒ½
            - **ä¸¦åˆ—æ•°**: 1-10ä¸¦åˆ—ã§èª¿æ•´å¯èƒ½
            - **é€²æ—è¡¨ç¤º**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç¢ºèª
            - **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å€‹åˆ¥ãƒãƒƒãƒã®ã‚¨ãƒ©ãƒ¼å‡¦ç†
            """)


if __name__ == "__main__":
    main()
