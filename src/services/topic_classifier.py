"""LLMãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã‚µãƒ¼ãƒ“ã‚¹"""

from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any

import streamlit as st

from schema.classification import ClassificationResult
from services.llm import LLMClient


class LLMTopicClassifier:
    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4o",
        batch_size: int = 25,
        max_workers: int = 3,
    ):
        """å…±é€šLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã®Structured Outputã‚’å–å¾—"""
        self.llm_client = LLMClient(api_key=api_key, model=model)
        self.batch_size = batch_size
        self.max_workers = max_workers

    def _create_topic_definitions(self, topics_data: dict[str, Any]) -> str:
        """ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã®ä½œæˆ"""
        topic_info = []
        for topic in topics_data.get("topics", []):
            topic_str = f"ãƒˆãƒ”ãƒƒã‚¯{topic['id']}: {topic['name']}\n"
            topic_str += f"  èª¬æ˜: {topic['description']}\n"
            topic_str += f"  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(topic['keywords'])}\n"

            if topic.get("subtopics"):
                topic_str += "  ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯:\n"
                for subtopic in topic["subtopics"]:
                    topic_str += f"    {subtopic['id']}: {subtopic['name']}\n"
                    topic_str += f"      èª¬æ˜: {subtopic['description']}\n"
                    topic_str += (
                        f"      ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(subtopic['keywords'])}\n"
                    )

            topic_info.append(topic_str)

        return "\n\n".join(topic_info)

    def _classify_batch(
        self,
        batch_texts: list[str],
        batch_start_index: int,
        topic_definitions: str,
        data_description: str = "",
    ) -> ClassificationResult | None:
        """ãƒãƒƒãƒã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡ã™ã‚‹"""
        # ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®ä½œæˆï¼ˆãƒãƒƒãƒå†…ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ï¼‰
        text_list = "\n".join([f"{i}: {text}" for i, text in enumerate(batch_texts)])

        # ãƒ‡ãƒ¼ã‚¿èª¬æ˜éƒ¨åˆ†ã®æ§‹ç¯‰
        data_context = ""
        if data_description.strip():
            data_context = f"""
ãƒ‡ãƒ¼ã‚¿ã®èƒŒæ™¯ãƒ»èª¬æ˜:
{data_description.strip()}

"""

        prompt = f"""
ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯å®šç¾©ã«åŸºã¥ã„ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

{data_context}ãƒˆãƒ”ãƒƒã‚¯å®šç¾©:
{topic_definitions}

ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆï¼ˆ{len(batch_texts)}ä»¶ï¼‰:
{text_list}

åˆ†é¡ãƒ«ãƒ¼ãƒ«:
1. **å¿…é ˆ**: å…¨ã¦ã®{len(batch_texts)}ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹0ã‹ã‚‰{len(batch_texts) - 1}ã¾ã§ï¼‰ã‚’å¿…ãšåˆ†é¡ã—ã¦ãã ã•ã„
2. ãƒ†ã‚­ã‚¹ãƒˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯0ã‹ã‚‰{len(batch_texts) - 1}ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„
3. ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ãã ã•ã„
4. ä¿¡é ¼åº¦ã‚’0-1ã®æ•°å€¤ã§æŒ‡å®šã—ã¦ãã ã•ã„
5. åˆ†é¡ç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„
6. æœ€ã‚‚é©åˆ‡ãªãƒˆãƒ”ãƒƒã‚¯ã‚’é¸æŠã—ã¦ãã ã•ã„
7. åˆ†é¡ã§ããªã„ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯ã€ã€Œãã®ä»–ã€ã¨ã„ã†ãƒˆãƒ”ãƒƒã‚¯ãƒ»ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã«å‰²ã‚Šå½“ã¦ã¦ãã ã•ã„
8. ãƒ‡ãƒ¼ã‚¿ã®èƒŒæ™¯ãƒ»èª¬æ˜ã‚’å‚è€ƒã«ã—ã¦ã€æ–‡è„ˆã«é©ã—ãŸåˆ†é¡ã‚’è¡Œã£ã¦ãã ã•ã„

é‡è¦: å¿…ãš{len(batch_texts)}ä»¶å…¨ã¦ã®åˆ†é¡çµæœã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""

        system_message = "ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ”ãƒƒã‚¯ã«åˆ†é¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸå…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿…ãšåˆ†é¡ã—ã¦ãã ã•ã„ã€‚åˆ†é¡çµæœã®æ•°ã¯å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæ•°ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"

        result = self.llm_client.structured_completion(
            prompt=prompt,
            response_format=ClassificationResult,
            system_message=system_message,
            temperature=0.1,
        )

        try:
            # çµæœã®æ¤œè¨¼
            if result and result.classifications:
                # åˆ†é¡çµæœã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯
                if len(result.classifications) != len(batch_texts):
                    st.warning(
                        f"âš ï¸ ãƒãƒƒãƒåˆ†é¡ã§æœŸå¾…ä»¶æ•°ã¨ç•°ãªã‚‹çµæœ: æœŸå¾…{len(batch_texts)}ä»¶ã€å®Ÿéš›{len(result.classifications)}ä»¶"
                    )

                # ãƒãƒƒãƒå†…ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¨ä½“ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«èª¿æ•´
                for classification in result.classifications:
                    classification.text_index += batch_start_index

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                indices = [cls.text_index for cls in result.classifications]
                if len(set(indices)) != len(indices):
                    st.warning("âš ï¸ ãƒãƒƒãƒåˆ†é¡ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é‡è¤‡ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

                return result
            else:
                st.error(
                    f"ãƒãƒƒãƒåˆ†é¡ã§ç©ºã®çµæœãŒè¿”ã•ã‚Œã¾ã—ãŸï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º: {len(batch_texts)}ï¼‰"
                )
                return None

        except Exception as e:
            st.error(f"ãƒãƒƒãƒåˆ†é¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return None

    def classify_texts_parallel(
        self,
        texts: list[str],
        topics_data: dict[str, Any],
        progress_callback=None,
        data_description: str = "",
    ) -> ClassificationResult | None:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸¦åˆ—ã§ãƒãƒƒãƒåˆ†é¡ã™ã‚‹"""

        topic_definitions = self._create_topic_definitions(topics_data)

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒã«åˆ†å‰²
        batches = []
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i : i + self.batch_size]
            batches.append((batch_texts, i))

        all_classifications = []
        completed_batches = 0
        failed_batches = []

        # ä¸¦åˆ—å‡¦ç†ã§ãƒãƒƒãƒã‚’å‡¦ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å…¨ãƒãƒƒãƒã®ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
            future_to_batch = {
                executor.submit(
                    self._classify_batch,
                    batch_texts,
                    start_index,
                    topic_definitions,
                    data_description,
                ): (batch_texts, start_index)
                for batch_texts, start_index in batches
            }

            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
            for future in as_completed(future_to_batch):
                batch_texts, start_index = future_to_batch[future]
                try:
                    result = future.result()
                    if result and result.classifications:
                        all_classifications.extend(result.classifications)
                        if progress_callback:
                            progress_callback(
                                int((completed_batches + 1) / len(batches) * 100),
                                f"ãƒãƒƒãƒ {completed_batches + 1}/{len(batches)} å®Œäº† ({len(result.classifications)}ä»¶åˆ†é¡)",
                            )
                    else:
                        failed_batches.append((start_index, len(batch_texts)))
                        if progress_callback:
                            progress_callback(
                                int((completed_batches + 1) / len(batches) * 100),
                                f"ãƒãƒƒãƒ {completed_batches + 1}/{len(batches)} å¤±æ•—",
                            )

                    completed_batches += 1

                except Exception as e:
                    failed_batches.append((start_index, len(batch_texts)))
                    st.error(
                        f"ãƒãƒƒãƒå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆé–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {start_index}ï¼‰: {str(e)}"
                    )
                    completed_batches += 1

        # å¤±æ•—ã—ãŸãƒãƒƒãƒã®æƒ…å ±ã‚’è¡¨ç¤º
        if failed_batches:
            st.warning(f"âš ï¸ {len(failed_batches)}å€‹ã®ãƒãƒƒãƒã§åˆ†é¡ã«å¤±æ•—ã—ã¾ã—ãŸ:")
            for start_idx, batch_size in failed_batches:
                st.write(f"  - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {start_idx} ã‹ã‚‰ {batch_size} ä»¶")

        # çµæœã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«ã‚½ãƒ¼ãƒˆ
        all_classifications.sort(key=lambda x: x.text_index)

        # åˆ†é¡çµæœã®çµ±è¨ˆ
        expected_total = len(texts)
        actual_total = len(all_classifications)

        if progress_callback:
            progress_callback(100, f"å®Œäº†: {actual_total}/{expected_total} ä»¶åˆ†é¡")

        st.info(
            f"ğŸ“Š åˆ†é¡å®Œäº†: {actual_total}/{expected_total} ä»¶ ({actual_total / expected_total * 100:.1f}%)"
        )

        return ClassificationResult(classifications=all_classifications)

    def classify_texts(
        self, texts: list[str], topics_data: dict[str, Any], data_description: str = ""
    ) -> ClassificationResult | None:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®å¾“æ¥ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆéæ¨å¥¨ï¼‰"""
        return self.classify_texts_parallel(
            texts, topics_data, data_description=data_description
        )
